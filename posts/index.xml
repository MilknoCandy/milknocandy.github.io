<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on PaperMoon&#39;s blog</title>
    <link>https://milknocandy.github.io/posts/</link>
    <description>Recent content in Posts on PaperMoon&#39;s blog</description>
    <image>
      <title>PaperMoon&#39;s blog</title>
      <url>https://milknocandy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://milknocandy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.154.3</generator>
    <language>en</language>
    <lastBuildDate>Tue, 20 Jan 2026 21:00:43 +0800</lastBuildDate>
    <atom:link href="https://milknocandy.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LoRA Variants Surveys</title>
      <link>https://milknocandy.github.io/posts/2026-01-16-lora/</link>
      <pubDate>Fri, 16 Jan 2026 00:09:30 +0800</pubDate>
      <guid>https://milknocandy.github.io/posts/2026-01-16-lora/</guid>
      <description>&lt;h2 id=&#34;1-timeline-order&#34;&gt;1 Timeline Order&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Summarize the literature reviewed in chronological order.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;2023&#34;&gt;2023&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸ“ã€&lt;em&gt;&lt;strong&gt;EMNLP 2023 - Main&lt;/strong&gt;&lt;/em&gt;ã€‘- Sparse Low-rank Adaptation of Pre-trained Language Models (&lt;em&gt;Tsinghua University, The University of Chicago&lt;/em&gt;)&lt;/p&gt;
&lt;div class=&#34;highlight-box default&#34;&gt;
    &lt;div class=&#34;box-content&#34;&gt;
        &lt;p&gt;&lt;strong&gt;Subject:&lt;/strong&gt; Adaptive Rank Selection&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Standard LoRA uses a fixed, inflexible rank (hyperparameter $ r
 $), requiring expensive manual tuning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Core Idea:&lt;/strong&gt; Make the rank learnable rather than fixed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gating:&lt;/strong&gt; Introduces an optimizable gating unit to the low-rank matrices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization:&lt;/strong&gt; Uses proximal gradient methods to update the gates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamics:&lt;/strong&gt; Prunes less important ranks during training automatically.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; Eliminates discrete rank search; the model discovers its own optimal rank structure.&lt;/li&gt;
&lt;/ul&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;figure &gt;
    &lt;img src=&#34;1-sora.png&#34; alt=&#34;SoRA&#34; /&gt;&lt;figcaption&gt;
        &lt;span class=&#34;auto-fig-title&#34;&gt;SoRA&lt;/span&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>â€‹Designing Bert for Convolutional Networks</title>
      <link>https://milknocandy.github.io/posts/2025-08-28-spark/</link>
      <pubDate>Thu, 28 Aug 2025 20:47:43 +0800</pubDate>
      <guid>https://milknocandy.github.io/posts/2025-08-28-spark/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;SparKï¼š&lt;a href=&#34;https://github.com/keyu-tian/SparK&#34;&gt;Designing Bert for Convolutional Networkss: Sparse and Hierarchical Masked Modeling&lt;/a&gt; (ICLR 2023 Spotlight)&lt;/p&gt;
&lt;p&gt;è®ºæ–‡ä»‹ç»ï¼š&lt;font style=&#34;color:rgb(38, 38, 38);&#34;&gt;&lt;/font&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV11s4y1M7qL/&#34;&gt;https://www.bilibili.com/video/BV11s4y1M7qL/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Bertç®—æ³•æ˜¯é®ä½æ•°æ®çš„ä¸€éƒ¨åˆ†ï¼Œç”¨æ¨¡å‹å»è¿›è¡Œé¢„æµ‹ï¼Œè¾¾åˆ°ä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ çš„æ•ˆæœã€‚è¿ç§»åˆ°å›¾åƒé¢†åŸŸä¸­çš„è§†è§‰Transformerçš„å·¥ä½œæ¯”å¦‚MAEï¼Œä½†æ˜¯ç›´æ¥å°†Transformeræ›¿æ¢ä¸ºå·ç§¯ç½‘ç»œåˆ™å‡ºç°é—®é¢˜ã€‚å¦‚ä¸‹å›¾ï¼Œzero-outingè¡¨ç¤ºç›´æ¥æ›¿æ¢ï¼š&lt;/p&gt;
&lt;!-- è¿™æ˜¯ä¸€å¼ å›¾ç‰‡ï¼Œocr å†…å®¹ä¸ºï¼šHIERARCHY APE MASKING EPOCH METHOD STD. LOSS ACC. 83.1 -1.0 NOT PRETRAINED 0.07 SPARK(OURS) 84.1 2 0.0 MASKED ONLY 1600 SPARSE X 3 83.2 0.06 ZERO-OUTING 1600 -0.9 MASKED ONLY ZERO-OUTING --&gt;
&lt;p&gt;
&lt;figure &gt;
    &lt;img src=&#34;fig1.png&#34; alt=&#34;&#34; /&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;å¯ä»¥çœ‹åˆ°åªæœ‰0.1ä¸ªç‚¹çš„æå‡ï¼Œæ˜¯å®Œå…¨æ— æ•ˆçš„ã€‚ä¸‹é¢æ˜¯ä½œè€…çš„åˆ†æã€‚&lt;/p&gt;
&lt;h2 id=&#34;ä¸ºä»€ä¹ˆå¤±è´¥&#34;&gt;ä¸ºä»€ä¹ˆå¤±è´¥ï¼Ÿ&lt;/h2&gt;
&lt;h3 id=&#34;é—®é¢˜1pixel-intensity-distribution-shift&#34;&gt;é—®é¢˜1ï¼šPixel Intensity Distribution Shift&lt;/h3&gt;
&lt;p&gt;Transformeråœ¨å¤„ç†patchesæ—¶ï¼Œåªè¦ä¿è¯æ˜¯éšæœºåˆ å»ä¸€äº›patchesï¼Œå¯ä»¥ä¿è¯åˆ é™¤çš„patcheså’Œå›¾åƒçš„åƒç´ åˆ†å¸ƒæ˜¯ä¸€è‡´çš„ã€‚è€Œå·ç§¯ç¥ç»ç½‘ç»œåˆ™ä¸èƒ½åˆ å»ä¸€äº›åƒç´ ï¼Œåªèƒ½æ˜¯å°†ä¸€äº›åƒç´ â€œæ¶‚é»‘â€æ¥æ¨¡æ‹Ÿä¸¢å¤±è¿™éƒ¨åˆ†åƒç´ çš„ä¿¡æ¯ã€‚&lt;/p&gt;
&lt;!-- è¿™æ˜¯ä¸€å¼ å›¾ç‰‡ï¼Œocr å†…å®¹ä¸ºï¼šCNN SPARSE CNN TRANSFORMER ENCODING PROCESS: PIXEL INTENSITY DATA DISTRIBUTION MA PROBABILITY BEFORE/AFTER MASKING: (A)DIRECTLY DROPPING (C)SPARSELY DROPPING (B)ZERO-OUTING (D) RAW INPUT --&gt;
&lt;p&gt;
&lt;figure &gt;
    &lt;img src=&#34;fig2.png&#34; alt=&#34;åƒç´ åˆ†å¸ƒã€‚æ¨ªè½´æ˜¯åƒç´ å¼ºåº¦ï¼Œçºµè½´æ˜¯åƒç´ å‡ºç°çš„é¢‘ç‡&#34; /&gt;&lt;figcaption&gt;
        &lt;span class=&#34;auto-fig-title&#34;&gt;åƒç´ åˆ†å¸ƒã€‚æ¨ªè½´æ˜¯åƒç´ å¼ºåº¦ï¼Œçºµè½´æ˜¯åƒç´ å‡ºç°çš„é¢‘ç‡&lt;/span&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-supervised Object-Centric Learning for Videos</title>
      <link>https://milknocandy.github.io/posts/2023-12-10-cutler/</link>
      <pubDate>Sun, 10 Dec 2023 11:35:33 +0800</pubDate>
      <guid>https://milknocandy.github.io/posts/2023-12-10-cutler/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;æ¥æºï¼š&lt;a href=&#34;https://openreview.net/group?id=NeurIPS.cc/2023/Conference&#34;&gt;NIPS 2023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;è®ºæ–‡åœ°å€ï¼š&lt;a href=&#34;http://arxiv.org/abs/2310.06907&#34;&gt;http://arxiv.org/abs/2310.06907&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä»£ç åœ°å€ï¼šâŒ&lt;/p&gt;
&lt;p&gt;ä½œè€…ä¸»é¡µï¼šäºŒä½œè°¢ä¼Ÿè¿ªä¸»é¡µ&lt;a href=&#34;https://weidixie.github.io/&#34;&gt;https://weidixie.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;é¡¹ç›®ä¸»é¡µï¼š&lt;a href=&#34;https://kuis-ai.github.io/solv/&#34;&gt;https://kuis-ai.github.io/solv/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ä»‹ç»&#34;&gt;ä»‹ç»&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;èƒŒæ™¯&lt;/strong&gt;ï¼š&lt;u&gt;æ— ç›‘ç£å¤šç›®æ ‡åˆ†å‰²&lt;/u&gt;å€ŸåŠ©è‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒä¸­å­¦ä¹ åˆ°çš„å¼ºåŠ›çš„è¯­ä¹‰ä¿¡æ¯å±•ç¤ºäº†æ˜¾è‘—çš„æ•ˆæœã€‚é€šå¸¸ä¹Ÿæ˜¯é€šè¿‡æ·»åŠ é¢å¤–çš„æ¨¡æ€ï¼ˆæ¯”å¦‚æ·±åº¦ã€åŠ¨ä½œï¼‰æ¥å¢å¼ºè§†é¢‘åºåˆ—çš„åˆ†å‰²ç»“æœã€‚ç„¶è€Œï¼Œåœ¨ _åˆæˆåºåˆ— _ä¸­è§‚å¯Ÿåˆ°çš„æ€§èƒ½æå‡&lt;u&gt;ä¾èµ–&lt;/u&gt;äºé¢å¤–ä¿¡æ¯çš„é²æ£’æ€§ï¼Œå¹¶ä¸èƒ½è½¬åŒ–ä¸ºæ›´å…·æŒ‘æˆ˜çš„çœŸå®ä¸–ç•Œåœºæ™¯ã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ä»»åŠ¡&lt;/strong&gt;ï¼šç»™å®šä¸€ä¸ªå¤æ‚åœºæ™¯çš„è§†é¢‘åºåˆ—ï¼Œç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªè§†è§‰ç³»ç»Ÿèƒ½å¤Ÿ&lt;u&gt;å‘ç°ã€è¿½è¸ªå’Œåˆ†å‰²&lt;/u&gt;å›¾åƒå¸§é‡Œçš„ç›®æ ‡ï¼Œå°†æ•°ç™¾ä¸‡çš„åƒç´ çš„è§†è§‰ä¿¡æ¯æŠ½è±¡ä¸º&lt;i&gt;è¯­ä¹‰éƒ¨åˆ†&lt;/i&gt;ã€‚ï¼ˆobject-centricè§†è§‰è¡¨å¾å­¦ä¹ ï¼‰&lt;/p&gt;
&lt;figure class=&#34;main-figure&#34;&gt;
  &lt;div class=&#34;side-by-side-wrapper grid-layout&#34;&gt;
    &lt;div class=&#34;side-item&#34; style=&#34;--w: 45%&#34;&gt;
      &lt;img src=&#34;1.gif&#34;&gt;
      &lt;p&gt;(a) Ground Truth&lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&#34;side-item&#34; style=&#34;--w: 45%&#34;&gt;
      &lt;img src=&#34;1-2.gif&#34;&gt;
      &lt;p&gt;(b) Prediction&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;!-- &lt;figcaption&gt;
    &lt;span class=&#34;auto-fig-title&#34;&gt;éå¯¹ç§°æ¯”ä¾‹å¯¹æ¯”&lt;/span&gt;
  &lt;/figcaption&gt; --&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;é¢†åŸŸçš„å‘å±•&lt;/strong&gt;ï¼šä»&lt;i&gt;åˆæˆå›¾åƒ&lt;/i&gt;å¼€å§‹ï¼Œè½¬å‘&lt;u&gt;in-the-wild&lt;/u&gt;å›¾åƒå’Œ&lt;u&gt;real-world&lt;/u&gt;è§†é¢‘ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨è‡ªç¼–ç å™¨è®­ç»ƒèŒƒå¼ï¼ˆå¦‚é‡å»ºè¾“å…¥ä¿¡å·ï¼Œå¸Œæœ›èƒ½åŸºäºæ•°æ®æˆ–ç»“æ„çš„å…ˆéªŒæ¥å°†&lt;u&gt;åŒºåŸŸåƒç´ &lt;/u&gt;åˆ†ç»„ä¸ºæœ‰è¯­ä¹‰æ„ä¹‰çš„å¯¹è±¡ï¼‰ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯¹å›¾åƒï¼šä½¿ç”¨æ¥æºäºé¢„è®­ç»ƒæ¨¡å‹çš„&lt;u&gt;ä½çº§ç‰¹å¾&lt;/u&gt;ï¼ˆå¦‚é¢œè‰²ã€è¯­ä¹‰ç‰¹å¾ç­‰ï¼‰æ¥ç¡®å®šåƒç´ åˆ°ç›®æ ‡çš„åˆ†é…&lt;/li&gt;
&lt;li&gt;å¯¹è§†é¢‘ï¼šé€šå¸¸ç»“åˆé¢å¤–çš„æ¨¡æ€ã€ä¿¡å·ï¼ˆå¦‚å…‰æµã€æ·±åº¦å›¾ï¼‰ï¼Œå¯ç›´æ¥ä»ä¸è¿ç»­æ€§è·å¾—åˆ†å‰²æ©ç &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;æå‡ºé—®é¢˜&#34;&gt;æå‡ºé—®é¢˜&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;ä½¿ç”¨é¢å¤–ä¿¡æ¯å¸¦æ¥çš„é—®é¢˜&lt;/strong&gt;ï¼šåœ¨è§†é¢‘ä¸­ä½¿ç”¨é¢å¤–çš„ä¿¡å·ä¼šå¢åŠ &lt;strong&gt;è®¡ç®—å¼€é”€&lt;/strong&gt;å’Œ&lt;strong&gt;è¯¯å·®ç´¯ç§¯&lt;/strong&gt;ã€‚æ¯”å¦‚å…‰æµä¿¡å·åœ¨å¤„ç†&lt;u&gt;é™æ€æˆ–å¯å˜å½¢&lt;/u&gt;ç‰©ä½“ä»¥åŠå¸§é—´&lt;u&gt;å¤§ä½ç§»&lt;/u&gt;æ—¶å¯èƒ½ä¼šäº§ç”Ÿé—®é¢˜ï¼Œè€Œæ·±åº¦å€¼åœ¨æ™®é€šè§†é¢‘ä¸­å¯èƒ½ä¸æ˜“è·å¾—ï¼Œåœ¨&lt;u&gt;ä½å…‰ç…§&lt;/u&gt;æˆ–&lt;u&gt;ä½å¯¹æ¯”åº¦&lt;/u&gt;ç¯å¢ƒä¸­å…¶ä¼°ç®—ä¹Ÿä¼šå—åˆ°å½±å“ã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;è¿‡åº¦åˆ†å‰²é—®é¢˜&lt;/strong&gt;ï¼šç”±äºè§†è§‰åœºæ™¯çš„å¤æ‚æ€§ï¼Œä½¿ç”¨å›ºå®šæ•°é‡çš„&lt;u&gt;slots&lt;/u&gt;å¯èƒ½å¯¼è‡´è¿‡åº¦åˆ†å‰²é—®é¢˜ï¼ˆover-segmentation issuseï¼‰ã€‚&lt;/p&gt;
&lt;h3 id=&#34;è§£å†³é—®é¢˜&#34;&gt;è§£å†³é—®é¢˜&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;ä½œè€…æ–¹æ³•&lt;/strong&gt;ï¼š&lt;strong&gt;é¦–æ¬¡&lt;/strong&gt;æå‡ºç”¨äº&lt;u&gt;çœŸå®ä¸–ç•Œåºåˆ—ä¸­å¤šç›®æ ‡åˆ†å‰²&lt;/u&gt;çš„å®Œå…¨æ— ç›‘ç£æ–¹æ³•ã€‚SOLVï¼Œä¸€ä¸ªèƒ½å¤Ÿå‘ç°çœŸå®ä¸–ç•Œè§†é¢‘åºåˆ—ä¸­å¤šä¸ªç›®æ ‡ä¸”ä¸ä½¿ç”¨é¢å¤–çš„æ¨¡æ€ä¿¡æ¯æˆ–ä»»ä½•ç±»ä¼¼å¼±ç›‘ç£æ–¹æ³•ï¼ˆæ¯”å¦‚ä½¿ç”¨ç¬¬ä¸€å¸§è¿›è¡Œåˆå§‹åŒ–ï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;æ–¹æ¡ˆ&lt;/strong&gt;ï¼šä½¿ç”¨è½´å‘ç©ºé—´-æ—¶éš™æ³¨æ„åŠ›ï¼ˆaxial spatial-temporal slot attentionsï¼‰&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;é¦–å…ˆå¯¹æ¯å¸§å†…ç©ºé—´åŒºåŸŸè¿›è¡Œåˆ†ç»„&lt;/li&gt;
&lt;li&gt;ç„¶åä½¿ç”¨æ¥è‡ªç›¸é‚»å¸§çš„äº¤äº’æ¥ä¸°å¯Œæ—¶éš™è¡¨ç¤ºï¼ˆslot representationsï¼‰&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;è®­ç»ƒç­–ç•¥&lt;/strong&gt;ï¼šmasked autoencoderï¼ˆMAEï¼‰è®­ç»ƒèŒƒå¼ã€‚ä¸¤ä¸ªä¼˜åŠ¿ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å……å½“ä¿¡æ¯ç“¶é¢ˆï¼ˆinformation bottleneckï¼‰ï¼Œè®©æ¨¡å‹è§‚å¯Ÿéƒ¨åˆ†åŒºåŸŸï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¹ é«˜çº§è¯­ä¹‰ç»“æ„ã€‚&lt;/li&gt;
&lt;li&gt;ç¼“è§£å†…å­˜é™åˆ¶ï¼Œæœ‰åŠ©äºæé«˜è®¡ç®—æ•ˆç‡&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;é’ˆå¯¹&lt;strong&gt;over-segmentation&lt;/strong&gt;é—®é¢˜ï¼šä½œè€…é€šè¿‡ä½¿ç”¨ç®€å•çš„èšç±»ç®—æ³•æ¥èåˆç›¸ä¼¼çš„slotsã€‚&lt;/p&gt;
&lt;p&gt;æ€»çš„æ¥è¯´ï¼Œè´¡çŒ®å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æå‡ºä¸€ä¸ªåœ¨çœŸå®ä¸–ç•Œè§†é¢‘ä¸Šçš„è‡ªç›‘ç£å¤šç›®æ ‡åˆ†å‰²æ¨¡å‹ï¼Œä½¿ç”¨axial spatial-temporal slots attentionï¼Œèƒ½æœ‰æ•ˆåœ°å°†å…·æœ‰ç›¸ä¼¼ç‰¹æ€§çš„è§†è§‰åŒºåŸŸè¿›è¡Œåˆ†ç»„ï¼Œè€Œä¸éœ€è¦ä½¿ç”¨&lt;u&gt;é¢å¤–çš„ä¿¡å·&lt;/u&gt;ã€‚&lt;/li&gt;
&lt;li&gt;å±•ç¤ºäº†ä¸€ä¸ªåŸºäºæ©ç ç‰¹å¾é‡å»ºçš„object-centricå­¦ä¹ æ–¹å¼ä»¥åŠslotèåˆæ–¹æ³•ã€‚&lt;/li&gt;
&lt;li&gt;MOVi-Eå’ŒYoutube-VIS 2019æ•°æ®é›†ä¸Šçš„SOTAä»¥åŠDAVIS2017æ•°æ®é›†ä¸Šçš„å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;slotå³è§†é¢‘ä¸­çš„å„ç‰©ä½“å¯¹è±¡ï¼Œè§ä¸‹å›¾ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;
&lt;figure &gt;
    &lt;img src=&#34;2.png&#34; alt=&#34;Source from: Conditional object-centric learning from video&#34; /&gt;&lt;figcaption&gt;
        &lt;span class=&#34;auto-fig-title&#34;&gt;Source from: Conditional object-centric learning from video&lt;/span&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;ç›¸å…³å·¥ä½œ&#34;&gt;ç›¸å…³å·¥ä½œ&lt;/h3&gt;
&lt;h4 id=&#34;object-centric-learning&#34;&gt;Object-centric Learning&lt;/h4&gt;
&lt;p&gt;å›¾åƒå’Œè§†é¢‘çš„object-centricæ— ç›‘ç£è¡¨å¾å­¦ä¹ ç°æœ‰å‡ ç§è§£å†³åŠæ³•ï¼š&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
